<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.2">
<link rel="alternate" type="application/rss+xml" href="/portfolio/blog/rss.xml" title="Nick Silva Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/portfolio/blog/atom.xml" title="Nick Silva Blog Atom Feed"><title data-react-helmet="true">Understanding Latent Semantic Analysis | Nick Silva</title><meta data-react-helmet="true" property="og:title" content="Understanding Latent Semantic Analysis | Nick Silva"><meta data-react-helmet="true" name="description" content="Introduction to LSA:"><meta data-react-helmet="true" property="og:description" content="Introduction to LSA:"><meta data-react-helmet="true" property="og:url" content="https://silva-nick.github.io/portfolio/blog/lsa"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><link data-react-helmet="true" rel="shortcut icon" href="/portfolio/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://silva-nick.github.io/portfolio/blog/lsa"><link data-react-helmet="true" rel="alternate" href="https://silva-nick.github.io/portfolio/blog/lsa" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://silva-nick.github.io/portfolio/blog/lsa" hreflang="x-default"><link rel="stylesheet" href="/portfolio/assets/css/styles.9a26afa5.css">
<link rel="preload" href="/portfolio/assets/js/runtime~main.da92b562.js" as="script">
<link rel="preload" href="/portfolio/assets/js/main.ce1d30e5.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#main" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/portfolio"><b class="navbar__title">Home</b></a><a class="navbar__item navbar__link" href="/portfolio/docs/about">More Info</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/portfolio/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/silva-nick" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="react-toggle displayOnlyInLargeViewport_GrZ2 react-toggle--disabled"><div class="react-toggle-track" role="button" tabindex="-1"><div class="react-toggle-track-check"><span class="toggle_71bT">üåú</span></div><div class="react-toggle-track-x"><span class="toggle_71bT">üåû</span></div><div class="react-toggle-thumb"></div></div><input type="checkbox" class="react-toggle-screenreader-only" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/portfolio"><b class="navbar__title">Home</b></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/portfolio/docs/about">More Info</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/portfolio/blog">Blog</a></li><li class="menu__list-item"><a href="https://github.com/silva-nick" target="_blank" rel="noopener noreferrer" class="menu__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_2ahu thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_2hhb margin-bottom--md">All posts</div><ul class="sidebarItemList_2xAf"><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/portfolio/blog/docusaurus-plugins">How to create a custom plugin for Docusaurus</a></li><li class="sidebarItem_2UVv"><a aria-current="page" class="sidebarItemLink_1RT6 sidebarItemLinkActive_12pM" href="/portfolio/blog/lsa">Understanding Latent Semantic Analysis</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/portfolio/blog/big-launch">Big Launch!</a></li><li class="sidebarItem_2UVv"><a class="sidebarItemLink_1RT6" href="/portfolio/blog/blog-example">Hello World!</a></li></ul></nav></aside><main class="col col--7"><article><header><h1 class="blogPostTitle_GeHD">Understanding Latent Semantic Analysis</h1><div class="blogPostData_291c margin-vert--md"><time datetime="2021-07-04T00:00:00.000Z">July 4, 2021</time> ¬∑ 15 min read</div><div class="avatar margin-vert--md"><a href="https://github.com/silva-nick" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img src="https://avatars.githubusercontent.com/u/39960606?v=4" alt="Nick Silva"></a><div class="avatar__intro"><div class="avatar__name"><a href="https://github.com/silva-nick" target="_blank" rel="noopener noreferrer">Nick Silva</a></div><small class="avatar__subtitle">College Rising Junior</small></div></div></header><div class="markdown"><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction-to-lsa"></a>Introduction to LSA:<a class="hash-link" href="#introduction-to-lsa" title="Direct link to heading">#</a></h2><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="lsa"></a>LSA<a class="hash-link" href="#lsa" title="Direct link to heading">#</a></h4><p>A classical problem in the field of natural language processing (NLP) is that of finding and matching semantic meaning from large bodies of text. Some classical solutions involve stemming, controlled vocabularies, and using thesauri <sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>. A different approach to this problem is Latent Semantic Analysis (LSA). LSA is an application of dimension reduction to the fields of natural language processing and information retrieval <sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>. Mathematically, it involves (1) creating a vector encoding of a large body of text, (2) computing a truncated singular value decomposition on the matrix, and (3) retrieving word and vector similarity in the dimension-reduced space. LSA is a well-studied model that remains an important tool and benchmark in the task of semantic similarity.</p><p>Mathematically, LSA finds a singular value decomposition for a term-document matrix. The term-document matrix can be constructed in varying ways based off of different document encoding methods. We can signify this matrix A(m*n) for m terms and n documents. The singular value decomposition of A is given by,A=U*sigma*V^T where sigma gives the sorted singular values of A and U and V are the decomposed sides of the term-document matrix. In a truncated SVD, we reduce the size of these matricies by taking only the k largest singular values and their respective vectors in U and V. As a result we find A_k, a rank k representation of our original term-document matrix.</p><p>Thus, the assumption of LSA is that semantically similar terms will be physically close together in the dimension-reduced space. Then, taking advantage of this distance, we can apply different algorithms to extract relationships between the documents. Moreover, the authors provide detailed argumentation as to why this type of dimension reduction parallels that used in the brain <sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>.</p><p>In this report, we look to describe, improve, and apply LSA. To begin, we provide a survey of the application of LSA to different NLP tasks and an experimental approach to understanding LSA in these settings. Next, we seek to improve and test different aspects of LSA and provide suggestions for some of the different variations available under the broader LSA framework. Finally, we attempt to prove the application of LSA in modern deep learning NLP models by inserting it as a re-weighting layer in the <code>deepspeech</code> pipeline.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="different-lsa-tasks"></a>Different LSA Tasks<a class="hash-link" href="#different-lsa-tasks" title="Direct link to heading">#</a></h4><p>LSA, although frequently applied to the task of semantic indexing, can also be applied to other NLP tasks. By capturing the dimension-reduced space, we are better able to cluster and make predictions on the data. Some of these applications include: sentiment analysis, topic grouping, and document classification. LSA&#x27;s ability to reduce the dimensionality of the data while retaining much of its latent meaning also makes it a useful tool in a diverse group NLP pipelines run on large datasets.</p><p>The application of LSA for sentiment analysis epitomizes its utility across a wide variety of NLP tasks. Generally, sentiment analysis refers to the task of extracting opinion or emotional data from text or image data. Modern semantic analysis models are often trained on huge labeled datasets of topic specific text. Recent models have become accurate enough to become useful models for guiding business decisions <sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>.</p><p>In this experiment, we will be using the <em>Twitter US Airline Sentiment</em> dataset accessed via Kaggle <sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>. As described by the original source, this dataset is,</p><blockquote><p>A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as &quot;late flight&quot; or &quot;rude service&quot;).</p></blockquote><p>There exists some pre existing literature on the use of LSA for sentiment analysis. Wang and Wan <sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup> suggest a six part ML pipeline that involves cleaning the data, computing the SVD, deciding the polarity of the words, combining the word orientation, and finally running SVM classification on the resulting data. Using this method they were able to achieve reasonably accurate results.</p><p>To simplify the method, we will run the pipeline following:</p><ol><li>Clean data and find vector encodings.</li><li>Find a dimension reduced SVD for all the data.</li><li>Run kNN-Classification on the resulting dimension reduced SVD.</li></ol><p>In running this pipeline with eight neighbors and reducing the term-document matrix to a rank 20 SVD, the algorithm achieved a classification accuracy of 70%. Obviously, this highly naive method for sentiment analysis failed to classify the tweets at a high rate. The sentiment breakdown of the original tweets was 63% negative, 16% positive, and 21% neutral. Therefore, the model did marginally better than had it always classified tweets as negative.</p><p><img alt="sentiment" src="/portfolio/assets/images/sentiment-750b472927108d0898224d222ebeace2.png"></p><p>A similar task that utilizes LSA dimension reduced matrices is topic modeling. Topic modelling is an unsupervised learning method that is used to cluster documents based on their textual context. This definition is intentionally vague as topic modeling describes a class of algorithms and techniques applied to many different more specific NLP tasks. For example, the dimension-reduced clusters found using a topic modeling algorithm, when labels are provided, can be used to classify documents. The ability of these models to access latent information apparent in the documents makes them easy to apply to real-world problems. Po and Bergamaschi report a simple four stage method to extract movie recommendation data from plot summaries <sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup>. The concept of topic modelling will be discussed in more detail in the <em>Improving LSA</em> section of this report.</p><p>Finally, among the most fundamental tasks solutions of which frequently utilize LSA is indexing and information retrieval. The final section of this report will focus on applying LSI (latent semantic indexing, synonymous to LSA) to improve pretrained STT models. The original LSI paper, <em>Deerwester et. al</em> <sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup>, provides the following methods of document-document and term-term comparison. After having computed A_k=U_k*sigma_k*V^T_k, we find the term-to-term product matrix A*A^T=U_k*sigma^2_k*U^T_k and the document-to-document product matrix A^TA=V_k*sigma^2_k*V^T_k. The i,j value of this matrix represents the similarity score between the i*text{th} and j*text{th} term/document in the original term-document matrix. Finally, the authors provide a metric for creating a dimension-reduced representation of a document that doesn&#x27;t exist in the original dataset. For original vector encoding of document p equal to X_p we can find a dimension reduced version under the constraint that transforming a vector in the matrix returns itself, V_p=X_p^T*U*sigma^-1</p><p>In these methods, LSA takes advantage of the distributional hypothesis which, in the context of linguistics, states that the semantic meaning of words in text can be understood by observing the context the words are found in <sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup>. In applying this hypothesis, we are able to generate a thesaurus (in the form of the term-to-term product matrix) from only the latent features of documents.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="improving-lsa"></a>Improving LSA:<a class="hash-link" href="#improving-lsa" title="Direct link to heading">#</a></h2><p>In this section, we examine and experiment on small adaptations to the LSA framework. All experiments will be performed on the topic classification/ modeling task, using the 20 newsgroup dataset <sup id="fnref-10"><a href="#fn-10" class="footnote-ref">10</a></sup>. To reduce the datasets size for ease, we selected 5 of the 20 topics: atheism, graphics, car reviews, baseball, and space. These experiments will also replace traditional SVD with SKLearn&#x27;s Randomized SVD (through 6 iterations) which is an implementation of Halko et. al 2009 <sup id="fnref-11"><a href="#fn-11" class="footnote-ref">11</a></sup>.</p><p>To begin, we performed a traditional LSA on the dataset. The kNN classifier (k=5) run on a rank 100 truncated SVD had an accuracy of 64%. We will use this number as our benchmark for future tests.</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#bfc7d5"><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># 20 graphics terms with largest weight</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;edu&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;lines&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;subject&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;graphics&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;organization&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;image&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;com&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;university&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">\</span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">     </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;jpeg&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;posting&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;host&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;file&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;nntp&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;software&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;data&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;use&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> \</span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">         </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;writes&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;images&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;files&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;know&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>Across the experiments we find that the document encoding, ceteris paribus, had marginal affect on the model&#x27;s accuracy. The classifier used however, made a major difference; Naive Bayes and SVM appeared as two viable options. In the final section, we find that all other attempted methods of dimension reduction fail to be computationally feasible for the large matrices required.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="document-encodings"></a>Document Encodings<a class="hash-link" href="#document-encodings" title="Direct link to heading">#</a></h4><p>In the paper that introduced LSA as an NLP technique, the textual documents were encoded as count vectors. Additionally, the authors suggested performing a logarithmic transformation to regularize the vectors. However, it remains unclear whether the aforementioned encoding and transformation are appropriate in optimizing the model. In the next section, we lay out a framework and experimental results from routine experiments across different. Across all experiments we plan to use the same five topic subsample of the 20 news dataset reduced to 100 dimensions via SVD.</p><p>We compare two new vector encoding techniques: one-hot or binary encoding and TF-IDF. Likewise we compare different potential tranformations that can be applied to the standard word-count encoding. In each case, we ran the LSA model on the new term-document matrix and calculated the model&#x27;s accuracy. Count and TF-IDF emerged as the two best encodings but only one-hot and the exponentially transformed vectors showed significantly different performance. A summary of the results is plotted below.</p><p><img alt="vectors" src="/portfolio/assets/images/vecs-6eb5e6496591e409ae8ed60487f4eac5.png"></p><p>Overall, it appears that either TD-IFD or a transformed count vector serve as the highest performance encodings. That said, this experiment was only across one NLP task, one dataset, and didn&#x27;t include other, more modern vector encodings.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="classifiers"></a>Classifiers<a class="hash-link" href="#classifiers" title="Direct link to heading">#</a></h4><p>The final step in the topic classification task involves applying a classification algorithm on the dimension reduced representation of the textual data. In other areas of this report, kNN was used as the constant classification algorithm. However, kNN is a relatively naive classification algorithm and our results could potentially be improved by replacing the kNN classifier with other classes of algorithms. Under the same setting as the other experiments in this section, we test and compare the performance of the kNN classifier to that of multlinomial naive Bayes and linear SVM.</p><p>Although kNN provided an above-random classification performance, it was easily surpassed by the accuracy and generalization performance of the naive Bayes and linear SVM classifiers. Both of the newly introduced models achieved sub 10% error values on the test data. This result is visualized in the barr graph below.</p><p>![clf][(/img/blog/clf.png)]</p><p>Although this was a survey of just a small group of classifiers, our results show the importance of classifier selection. Moreover, it exemplifies the power of the LSA model; by reducing dimensionality of the model, we are able to linearly separate the topics into their respective groups and do so with a hight level of accuracy.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="ideal-dimensions"></a>Ideal Dimensions<a class="hash-link" href="#ideal-dimensions" title="Direct link to heading">#</a></h4><p>Another common problem when applying LSA is that of specifying the ideal number of dimensions of the transformed encoding. Being able to reduce the number of dimensions of the data, the word vectors in which could be tens of thousands of integers long, is the fundamental goal and property of the truncated SVD. However, removing too many dimensions could result in a representation that doesn&#x27;t manage to accurately describe the dataset. The ideal number of output dimensions is a product of the input data, vector length, and sample size. In this section, we hope to provide a simple quick method to optimize the final output truncation.</p><p>The figure below, accessed from <em>An Introduction to LSA</em>, shows this phenomenon visually <sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>. In this experiment, they used LSA to find synonyms on a TOEFL dataset. They found that they maximized correctness when the data was reduced to ~300 dimensions.</p><p><img alt="LSA Mountain" src="/portfolio/assets/images/mountain-a8ad3b437fbd8e2eab1336f4c96c6830.png"></p><p>However, 300 is not a universal number, nor is 100 (the value originally suggested by the authors). Instead, careful tuning is important to ensure that the number of dimensions is correct. Here we provide a simple method to find a locally optimal number of dimension drawing ideas from SGD. In this case, however, we are able to only calculate the SVD once and upon each update remove more vectors from the decomposition.</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly python"><pre tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">best_dim</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">train_X</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> train_Y</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> threshold </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">.01</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Decide the number of components:</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    n_components </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">100</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">if</span><span class="token plain"> train_X</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">shape</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token number" style="color:rgb(247, 140, 108)">0</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">&gt;</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">5000</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        n_components </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">1000</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Run SVD on the training data</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    U</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> sigma</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> VT </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> randomized_svd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">train_encoding</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">                                n_components</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">1000</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">                                n_iter</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">6</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">                                random_state</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token boolean" style="color:rgb(255, 88, 116)">None</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Train a kNN classifier on the truncated SVD</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    clf_knn </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> KNeighborsClassifier</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">fit</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">VT</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">T</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> train_Y</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Also customizable classifier, params</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Test data transformed</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    test_trans </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> test_encoding</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">T @ U @ np</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">diag</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">np</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">power</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">sigma</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Compute score</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    last </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> clf_knn</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">score</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">test_trans</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> test_Y</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    delta </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token number" style="color:rgb(247, 140, 108)">.1</span><span class="token plain">         </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Arbitrary</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    increasing </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token boolean" style="color:rgb(255, 88, 116)">False</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">while</span><span class="token plain"> delta </span><span class="token operator" style="color:rgb(137, 221, 255)">&gt;</span><span class="token plain"> threshold</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        n_components </span><span class="token operator" style="color:rgb(137, 221, 255)">+=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token builtin" style="color:rgb(130, 170, 255)">int</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> n_components \</span><span class="token operator" style="color:rgb(137, 221, 255)">*</span><span class="token plain"> delta</span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Train a kNN classifier on the truncated SVD</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        clf_knn </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> KNeighborsClassifier</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">fit</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">VT</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain">n_components</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">T</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> train_Y</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Test data transformed</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        test_trans </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> test_encoding</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">T @ U</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain">n_components</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"> @ np</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">diag</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">np</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">power</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">sigma</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain">n_components</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Compute score</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        temp </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> clf_knn</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">score</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">test_trans</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> test_Y</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">last</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> n_components</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="dimension-reduction"></a>Dimension Reduction<a class="hash-link" href="#dimension-reduction" title="Direct link to heading">#</a></h4><p>In traditional LSA, the SVD is used to compute a lower dimension representation of the text encoding matrix. Under the general methodology of LSA, however, this step may be feasibly be replaced with other dimension reduction techniques. In this section, we attempted to replace the SVD with other methods of dimension reduction. Specifically, we ran a standard LSA pipeline simply replacing SVD with ISOMAP and metric MDS. All tests were run on the five topic subsample of the 20 news dataset.</p><p>Performing this experiment exposed one of the important reasons why the truncated SVD works well in this situation; It is one of the few dimension reduction methods that is computationally feasible for the huge matrices used in NLP. Other methods, like MDS and ISOMAP are computationally infeasible for matrices as small as the 70,000 * 3,000 term-document matrix used in this experiment. Moreover, SVD allows for a k-rank dimension reduction while other methods often perform best in n-to-2 or n-to-3 situations.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="applying-lsa"></a>Applying LSA:<a class="hash-link" href="#applying-lsa" title="Direct link to heading">#</a></h2><blockquote><p>I was not able to finish the code for this section, however, I find the idea interesting and hope to do more research into STT models in the future.</p></blockquote><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="concept"></a>Concept<a class="hash-link" href="#concept" title="Direct link to heading">#</a></h4><p>The motivation of this section of this paper stems from the wide application of pretrained deep speech to text models (STT) in business applications. These models, although increasingly accurate, often struggle with generalization, make recurrent mistakes, and are difficult to tune to specific input data <sup id="fnref-12"><a href="#fn-12" class="footnote-ref">12</a></sup>. For example, some models trained for normal speech recognition struggle when introduced to rare words. However, if a professor or orator is speaking in the topic-specific language of their field, the models often miss-simplify the person&#x27;s speech resulting in decreased performance.</p><p>Most modern open source and commercial models have &quot;boost&quot; parameters or more advanced methods to allow businesses to fine tune models to their needs <sup id="fnref-13"><a href="#fn-13" class="footnote-ref">13</a></sup>. These tools, however, often have to be pre-set and provide a one-time reweighting to the model. In many use cases however, an ideal model may dynamically adjust its weights during use as the topic of the speech changes. This type of reweighting requires a lightweight computationally simple solution.</p><p>LSA provides a simple method that could be used to dynamically adjust STT model weights as the topic of the speech changes.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="experiment"></a>Experiment<a class="hash-link" href="#experiment" title="Direct link to heading">#</a></h4><p>To begin, we loaded the latest <code>deepspeech</code> library <sup id="fnref-14"><a href="#fn-14" class="footnote-ref">14</a></sup> into a docker environment with the standard pretrained English model. From empirical analysis, this model achieved 16% error on the Switchboard Hub5&#x27;00 dataset <sup id="fnref-15"><a href="#fn-15" class="footnote-ref">15</a></sup>. Next, to get a benchmark on the model&#x27;s unchanged state, we ran it on snippets of our dataset. The dataset in question is a collection of...</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="conclusion"></a>Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">#</a></h2><p>LSA is a simple application of dimension reduction that serves as a key item in pipelines for a diverse group of NLP tasks. Despite being first documented over thirty years ago, it remains a powerful tool across many tasks.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="references"></a>References:<a class="hash-link" href="#references" title="Direct link to heading">#</a></h2><div class="footnotes"><hr><ol><li id="fn-1">Dumais, S.T. (2004), Latent semantic analysis. Ann. Rev. Info. Sci. Tech., 38: 188-230. <a href="https://doi.org/10.1002/aris.1440380105" target="_blank" rel="noopener noreferrer">https://doi.org/10.1002/aris.1440380105</a><a href="#fnref-1" class="footnote-backref">‚Ü©</a></li><li id="fn-2">Dumais, S. T., Furnas, G. W., Landauer, T. K., &amp; Deerwester, S. (1988). Using latent semantic analysis to improve information retrieval. Proceedings of CHI&#x27;88 Conference on Human Factors in Computing Systems, 281‚Äì285.<a href="#fnref-2" class="footnote-backref">‚Ü©</a></li><li id="fn-3">Landauer, T. K., Foltz, P. W., &amp; Laham, D. <a href="http://lsa.colorado.edu/papers/dp1.LSAintro.pdf" target="_blank" rel="noopener noreferrer">http://lsa.colorado.edu/papers/dp1.LSAintro.pdf</a><a href="#fnref-3" class="footnote-backref">‚Ü©</a></li><li id="fn-4">Usman Malik, <a href="https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/" target="_blank" rel="noopener noreferrer">https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/</a><a href="#fnref-4" class="footnote-backref">‚Ü©</a></li><li id="fn-5">Kaggle, <a href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/crowdflower/twitter-airline-sentiment</a><a href="#fnref-5" class="footnote-backref">‚Ü©</a></li><li id="fn-6">Wang, Lan and Wan, Yuan. ‚ÄúSentiment Classification of Documents Based on Latent Semantic Analysis.‚Äù Advanced Research on Computer Education, Simulation and Modeling, 2011, pp. 356‚Äì61.<a href="#fnref-6" class="footnote-backref">‚Ü©</a></li><li id="fn-7">Bergamaschi, Sonia, and Po, Laura. ‚ÄúComparing LDA and LSA Topic Models for Content-Based Movie Recommendation Systems.‚Äù Web Information Systems and Technologies, 2015, pp. 247‚Äì63.<a href="#fnref-7" class="footnote-backref">‚Ü©</a></li><li id="fn-8">Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K. and Harshman, R. (1990), Indexing by latent semantic analysis. J. Am. Soc. Inf. Sci., 41: 391-407. <a href="https://doi.org/10.1002/(SICI)1097-4571(199009)41:6" target="_blank" rel="noopener noreferrer">https://doi.org/10.1002/(SICI)1097-4571(199009)41:6</a>&lt;391::AID-ASI1&gt;3.0.CO;2-9<a href="#fnref-8" class="footnote-backref">‚Ü©</a></li><li id="fn-9">Firth, J.R. (1957). &quot;A synopsis of linguistic theory 1930-1955&quot;. Studies in Linguistic Analysis: 1‚Äì32. Reprinted in F.R. Palmer, ed. (1968). Selected Papers of J.R. Firth 1952-1959. London: Longman.<a href="#fnref-9" class="footnote-backref">‚Ü©</a></li><li id="fn-10">SK-Learn, <a href="https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html" target="_blank" rel="noopener noreferrer">https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html</a><a href="#fnref-10" class="footnote-backref">‚Ü©</a></li><li id="fn-11">Halko, et al., 2009 <a href="https://arxiv.org/abs/0909.4061" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/0909.4061</a><a href="#fnref-11" class="footnote-backref">‚Ü©</a></li><li id="fn-12">Tits, No√© &amp; El Haddad, Kevin &amp; Dutoit, Thierry. (2020). Exploring Transfer Learning for Low Resource Emotional TTS. 10.1007/978-3-030-29516-5_5.<a href="#fnref-12" class="footnote-backref">‚Ü©</a></li><li id="fn-13">Google Cloud STT documentation, <a href="https://cloud.google.com/speech-to-text/docs/adaptation-model" target="_blank" rel="noopener noreferrer">https://cloud.google.com/speech-to-text/docs/adaptation-model</a><a href="#fnref-13" class="footnote-backref">‚Ü©</a></li><li id="fn-14">Mozilla Deep Speech, <a href="https://github.com/mozilla/DeepSpeech" target="_blank" rel="noopener noreferrer">https://github.com/mozilla/DeepSpeech</a><a href="#fnref-14" class="footnote-backref">‚Ü©</a></li><li id="fn-15">Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, &amp; Andrew Y. Ng. (2014). Deep Speech: Scaling up end-to-end speech recognition. <a href="https://arxiv.org/abs/1412.5567" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1412.5567</a>.<a href="#fnref-15" class="footnote-backref">‚Ü©</a></li></ol></div></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_3kfx"><div class="col"><b>Tags:</b><a class="margin-horiz--sm" href="/portfolio/blog/tags/blog">blog</a><a class="margin-horiz--sm" href="/portfolio/blog/tags/data-science">data science</a></div><div class="col margin-top--sm"><a href="https://github.com/silva-nick/portfolio/tree/main/blog/blog/2021-7-04-lsa.mdx" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_2_ui" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/portfolio/blog/docusaurus-plugins"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">¬´ How to create a custom plugin for Docusaurus</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/portfolio/blog/big-launch"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Big Launch! ¬ª</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction-to-lsa" class="table-of-contents__link">Introduction to LSA:</a></li><li><a href="#improving-lsa" class="table-of-contents__link">Improving LSA:</a></li><li><a href="#applying-lsa" class="table-of-contents__link">Applying LSA:</a></li><li><a href="#conclusion" class="table-of-contents__link">Conclusion</a></li><li><a href="#references" class="table-of-contents__link">References:</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">silva-nick</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/portfolio">Home</a></li><li class="footer__item"><a class="footer__link-item" href="/portfolio/docs/resume">Resume</a></li><li class="footer__item"><a href="https://github.com/&lt;GITHUB-USER&gt;" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li></ul></div><div class="col footer__col"><div class="footer__title">About</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/portfolio/docs/about">About me</a></li><li class="footer__item"><a class="footer__link-item" href="/portfolio/docs/timeline">Timeline</a></li><li class="footer__item"><a class="footer__link-item" href="/portfolio/blog">Blog</a></li></ul></div></div><div class="footer__bottom text--center"></div></div></footer></div>
<script src="/portfolio/assets/js/runtime~main.da92b562.js"></script>
<script src="/portfolio/assets/js/main.ce1d30e5.js"></script>
</body>
</html>